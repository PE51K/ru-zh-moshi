{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T18:57:28.231051Z",
     "start_time": "2025-03-07T18:57:26.310458Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Chinese character detection regex...\n",
      "Found 11817 JSON files to process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 11817/11817 [00:06<00:00, 1745.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Found 4240 conversations\n",
      "Files containing Chinese characters: 4182\n",
      "Removed 7569 conversations with duplicate first messages\n",
      "Removed 8 conversations exceeding duplicate message threshold\n",
      "Total removed conversations: 7577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving conversations: 100%|██████████| 4240/4240 [00:00<00:00, 15271.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed conversations saved to ../dataset/text_cleaned/data.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm  # For progress bar\n",
    "from num2words import num2words\n",
    "import dateparser  # For time-related text conversion\n",
    "import hashlib\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define the root directory containing all the JSON files\n",
    "root_dir = \"../dataset/text\"\n",
    "output_file = \"../dataset/text_cleaned/data.jsonl\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "# Символы, которые нужно удалить\n",
    "UNWANTED_SYMBOLS = [\"$\", \"#\", \"*\", \">\", \"<\", \"\\n\", \"-\"]\n",
    "\n",
    "# Define punctuation characters to normalize\n",
    "PUNCTUATION_CHARS = \",.!?;:…\"\n",
    "\n",
    "# Define sentence separators (after these, capital letters are allowed)\n",
    "SENTENCE_SEPARATORS = \".!?\\n\"\n",
    "\n",
    "# Словарь для замены числовых указателей на текстовые аналоги с запятой\n",
    "ORDINAL_MAP = {\n",
    "    \"1\": \"во-первых,\",\n",
    "    \"2\": \"; во-вторых,\",\n",
    "    \"3\": \"; в-третьих,\",\n",
    "    \"4\": \"; в-четвёртых,\",\n",
    "    \"5\": \"; в-пятых,\",\n",
    "    \"6\": \"; в-шестых,\",\n",
    "    \"7\": \"; в-седьмых,\",\n",
    "    \"8\": \"; в-восьмых,\",\n",
    "    \"9\": \"; в-девятых,\",\n",
    "    \"10\": \"; в-десятых,\",\n",
    "}\n",
    "\n",
    "# Chinese character detection code\n",
    "LHan = [[0x2E80, 0x2E99],    # Han # So  [26] CJK RADICAL REPEAT, CJK RADICAL RAP\n",
    "        [0x2E9B, 0x2EF3],    # Han # So  [89] CJK RADICAL CHOKE, CJK RADICAL C-SIMPLIFIED TURTLE\n",
    "        [0x2F00, 0x2FD5],    # Han # So [214] KANGXI RADICAL ONE, KANGXI RADICAL FLUTE\n",
    "        0x3005,              # Han # Lm       IDEOGRAPHIC ITERATION MARK\n",
    "        0x3007,              # Han # Nl       IDEOGRAPHIC NUMBER ZERO\n",
    "        [0x3021, 0x3029],    # Han # Nl   [9] HANGZHOU NUMERAL ONE, HANGZHOU NUMERAL NINE\n",
    "        [0x3038, 0x303A],    # Han # Nl   [3] HANGZHOU NUMERAL TEN, HANGZHOU NUMERAL THIRTY\n",
    "        0x303B,              # Han # Lm       VERTICAL IDEOGRAPHIC ITERATION MARK\n",
    "        [0x3400, 0x4DB5],    # Han # Lo [6582] CJK UNIFIED IDEOGRAPH-3400, CJK UNIFIED IDEOGRAPH-4DB5\n",
    "        [0x4E00, 0x9FC3],    # Han # Lo [20932] CJK UNIFIED IDEOGRAPH-4E00, CJK UNIFIED IDEOGRAPH-9FC3\n",
    "        [0xF900, 0xFA2D],    # Han # Lo [302] CJK COMPATIBILITY IDEOGRAPH-F900, CJK COMPATIBILITY IDEOGRAPH-FA2D\n",
    "        [0xFA30, 0xFA6A],    # Han # Lo  [59] CJK COMPATIBILITY IDEOGRAPH-FA30, CJK COMPATIBILITY IDEOGRAPH-FA6A\n",
    "        [0xFA70, 0xFAD9],    # Han # Lo [106] CJK COMPATIBILITY IDEOGRAPH-FA70, CJK COMPATIBILITY IDEOGRAPH-FAD9\n",
    "        [0x20000, 0x2A6D6],  # Han # Lo [42711] CJK UNIFIED IDEOGRAPH-20000, CJK UNIFIED IDEOGRAPH-2A6D6\n",
    "        [0x2F800, 0x2FA1D]]  # Han # Lo [542] CJK COMPATIBILITY IDEOGRAPH-2F800, CJK COMPATIBILITY IDEOGRAPH-2FA1D\n",
    "\n",
    "# Define Chinese punctuation characters - FIXED version without dash-ranges\n",
    "chinese_punctuation = \"，。！？；：\"\"''「」【】《》〈〉（）［］｛｝…～\"\n",
    "\n",
    "def build_re():\n",
    "    \"\"\"Build regex for Chinese character detection\"\"\"\n",
    "    print(\"Building Chinese character detection regex...\")\n",
    "    L = []\n",
    "    for i in LHan:\n",
    "        if isinstance(i, list):\n",
    "            f, t = i\n",
    "            f = chr(f)\n",
    "            t = chr(t)\n",
    "            L.append(f'{f}-{t}')\n",
    "        else:\n",
    "            L.append(chr(i))\n",
    "    \n",
    "    # Add Chinese punctuation\n",
    "    for char in chinese_punctuation:\n",
    "        L.append(re.escape(char))  # Properly escape each character\n",
    "    \n",
    "    RE = '[%s]' % ''.join(L)\n",
    "    return re.compile(RE, re.UNICODE)\n",
    "\n",
    "# Initialize Chinese character detection\n",
    "chinese_detector = build_re()\n",
    "\n",
    "def is_chinese_char(char):\n",
    "    \"\"\"Check if a character is Chinese (including punctuation)\"\"\"\n",
    "    return chinese_detector.match(char) is not None or char in chinese_punctuation\n",
    "\n",
    "def remove_pinyin_guides(text):\n",
    "    \"\"\"Remove romanized pronunciation guides like (zài jiàn) from the text\"\"\"\n",
    "    # Pattern to match content within parentheses that contains pinyin-like characters\n",
    "    pinyin_pattern = r'\\([a-zA-Z\\s\\u0300-\\u036Fàáèéìíòóùúüāēěīōūǎǐǒǔǚǜ]+\\)'\n",
    "    \n",
    "    # Remove the pinyin guides\n",
    "    cleaned_text = re.sub(pinyin_pattern, '', text)\n",
    "    \n",
    "    # Fix any double spaces created by the removal\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "def split_by_language(text):\n",
    "    \"\"\"\n",
    "    Split text into Chinese and Russian segments,\n",
    "    keeping consecutive Chinese characters together\n",
    "    \"\"\"\n",
    "    segments = []\n",
    "    \n",
    "    # Process text character by character\n",
    "    i = 0\n",
    "    while i < len(text):\n",
    "        # Find the start of a Chinese segment\n",
    "        if is_chinese_char(text[i]):\n",
    "            start = i\n",
    "            i += 1\n",
    "            \n",
    "            # Continue until we find a non-Chinese character\n",
    "            while i < len(text) and is_chinese_char(text[i]):\n",
    "                i += 1\n",
    "                \n",
    "            # Check if the next characters include punctuation that should stay with Chinese\n",
    "            while i < len(text) and text[i] in \"!,.?;:\":\n",
    "                i += 1\n",
    "                \n",
    "            segments.append({\"text\": text[start:i], \"lang\": \"zh-cn\"})\n",
    "        else:\n",
    "            # Find the end of the non-Chinese segment\n",
    "            start = i\n",
    "            i += 1\n",
    "            \n",
    "            while i < len(text) and not is_chinese_char(text[i]):\n",
    "                i += 1\n",
    "                \n",
    "            segments.append({\"text\": text[start:i], \"lang\": \"ru\"})\n",
    "    \n",
    "    # Merge adjacent segments of the same language\n",
    "    merged_segments = []\n",
    "    current_lang = None\n",
    "    current_text = \"\"\n",
    "    \n",
    "    for segment in segments:\n",
    "        if current_lang is None:\n",
    "            current_lang = segment[\"lang\"]\n",
    "            current_text = segment[\"text\"]\n",
    "        elif current_lang == segment[\"lang\"]:\n",
    "            current_text += segment[\"text\"]\n",
    "        else:\n",
    "            merged_segments.append({\"text\": current_text.strip(), \"lang\": current_lang})\n",
    "            current_lang = segment[\"lang\"]\n",
    "            current_text = segment[\"text\"]\n",
    "            \n",
    "    if current_text:\n",
    "        merged_segments.append({\"text\": current_text.strip(), \"lang\": current_lang})\n",
    "    \n",
    "    # Filter out empty segments and segments with only punctuation\n",
    "    filtered_segments = []\n",
    "    for seg in merged_segments:\n",
    "        # Skip empty segments\n",
    "        if not seg[\"text\"].strip():\n",
    "            continue\n",
    "            \n",
    "        # Skip segments with only punctuation\n",
    "        if all(char in \"-−------!,.?;: \" for char in seg[\"text\"]):\n",
    "            continue\n",
    "            \n",
    "        filtered_segments.append(seg)\n",
    "    \n",
    "    return filtered_segments\n",
    "\n",
    "# Функция для замены числовых указателей (например, '1.') на текстовые аналоги\n",
    "def replace_ordinal_bullets(text):\n",
    "    return re.sub(r'\\b(\\d+)\\.', lambda x: ORDINAL_MAP.get(x.group(1), x.group(0)), text)\n",
    "\n",
    "# Функция для конвертации чисел в текст\n",
    "def convert_numbers_to_words(text):\n",
    "    # Округляем числа, чтобы избежать фраз типа \"четыре целых ноль десятых\"\n",
    "    return re.sub(r'\\d+(\\.\\d+)?', lambda x: num2words(round(float(x.group())), lang=\"ru\"), text)\n",
    "\n",
    "# Функция для конвертации времени в текст\n",
    "def convert_time_to_text(text):\n",
    "    return re.sub(r'\\b\\d{1,2}:\\d{2}\\b', lambda x: dateparser.parse(x.group()).strftime(\"%I:%M %p\"), text)\n",
    "\n",
    "# Функция для удаления нежелательных символов\n",
    "def remove_unwanted_symbols(text):\n",
    "    for symbol in UNWANTED_SYMBOLS:\n",
    "        text = text.replace(symbol, \" \")\n",
    "    return text.strip()\n",
    "\n",
    "# Функция для замены нескольких пробелов на один\n",
    "def replace_multiple_spaces(text):\n",
    "    return re.sub(r'\\s{2,}', ' ', text)\n",
    "\n",
    "# Функция для удаления пробелов перед определёнными символами\n",
    "def remove_spaces_before_symbols(text, symbols=\",.!?;:\"):\n",
    "    # Регулярное выражение для поиска пробелов перед указанными символами\n",
    "    pattern = rf'\\s+([{re.escape(symbols)}])'\n",
    "    return re.sub(pattern, r'\\1', text)\n",
    "\n",
    "# NEW FUNCTION: Replace consecutive punctuation with last punctuation + space\n",
    "def normalize_punctuation(text):\n",
    "    \"\"\"Replace consecutive punctuation with only the last punctuation character + space\"\"\"\n",
    "    # Create a regex pattern for any character in PUNCTUATION_CHARS followed by spaces and punctuation\n",
    "    pattern = r'([' + re.escape(PUNCTUATION_CHARS) + r'][\\s' + re.escape(PUNCTUATION_CHARS) + r']+)'\n",
    "    \n",
    "    def replace_punct_sequence(match):\n",
    "        # Get the matched punctuation sequence\n",
    "        sequence = match.group(1)\n",
    "        # Take the last punctuation character\n",
    "        last_char = None\n",
    "        for char in reversed(sequence):\n",
    "            if char in PUNCTUATION_CHARS:\n",
    "                last_char = char\n",
    "                break\n",
    "        if last_char:\n",
    "            return last_char + \" \"\n",
    "        return sequence\n",
    "    \n",
    "    return re.sub(pattern, replace_punct_sequence, text)\n",
    "\n",
    "# NEW FUNCTION: Escape sentence splitters\n",
    "def escape_sentence_splitters(text):\n",
    "    \"\"\"Replace sentence splitters like '. ' with '\\. '\"\"\"\n",
    "    # Replace period + space with escaped period + space\n",
    "    text = re.sub(r'\\. ', r'\\. ', text)\n",
    "    # Same for other sentence ending punctuation\n",
    "    text = re.sub(r'! ', r'\\! ', text)\n",
    "    text = re.sub(r'\\? ', r'\\? ', text)\n",
    "    return text\n",
    "\n",
    "# NEW FUNCTION: Convert uppercase to lowercase except for sentence beginnings\n",
    "def normalize_capitalization(text):\n",
    "    \"\"\"\n",
    "    Convert uppercase characters to lowercase except for:\n",
    "    1. First letter of the text\n",
    "    2. First letter after sentence separators\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return text\n",
    "        \n",
    "    result = []\n",
    "    should_be_upper = True  # First character should be uppercase\n",
    "    \n",
    "    for i, char in enumerate(text):\n",
    "        if should_be_upper and char.isalpha():\n",
    "            result.append(char.upper())\n",
    "            should_be_upper = False\n",
    "        elif (i > 0 and text[i-1] in SENTENCE_SEPARATORS) and char.isalpha():\n",
    "            # If the previous character was a sentence separator, keep uppercase\n",
    "            result.append(char)\n",
    "            should_be_upper = False\n",
    "        else:\n",
    "            # Otherwise, make uppercase characters lowercase\n",
    "            if char.isupper():\n",
    "                result.append(char.lower())\n",
    "            else:\n",
    "                result.append(char)\n",
    "                \n",
    "        # If we find a sentence separator, the next alphabetic character should be uppercase\n",
    "        if char in SENTENCE_SEPARATORS:\n",
    "            should_be_upper = True\n",
    "    \n",
    "    return ''.join(result)\n",
    "\n",
    "# Основная функция обработки контента\n",
    "def process_content(content):\n",
    "    content = remove_unwanted_symbols(content)  # Удаление нежелательных символов\n",
    "    content = replace_multiple_spaces(content)  # Замена нескольких пробелов на один\n",
    "    content = replace_ordinal_bullets(content)  # Замена числовых указателей\n",
    "    content = convert_numbers_to_words(content)  # Конвертация чисел в текст\n",
    "    content = convert_time_to_text(content)  # Конвертация времени в текст\n",
    "    content = remove_spaces_before_symbols(content)  # Удаление пробелов перед символами\n",
    "    content = remove_pinyin_guides(content)  # Remove pinyin guides\n",
    "    content = normalize_punctuation(content)  # Normalize consecutive punctuation\n",
    "    content = escape_sentence_splitters(content)  # Escape sentence splitters\n",
    "    content = normalize_capitalization(content)  # NEW: Fix capitalization issues\n",
    "    return content\n",
    "\n",
    "# Process for Chinese language detection and segmentation\n",
    "def process_and_segment_content(content):\n",
    "    # First apply the standard text processing\n",
    "    processed_content = process_content(content)\n",
    "    \n",
    "    # Then detect language segments\n",
    "    segments = split_by_language(processed_content)\n",
    "    \n",
    "    return {\n",
    "        \"processed_content\": processed_content,\n",
    "        \"language_segments\": segments\n",
    "    }\n",
    "\n",
    "# Список всех обработанных разговоров\n",
    "all_conversations = []\n",
    "\n",
    "# Поиск всех JSON-файлов в директории (исключая system_prompt.json)\n",
    "file_paths = []\n",
    "for subdir, _, files in os.walk(root_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".json\") and file != \"system_prompt.json\":  # Исключение system_prompt.json\n",
    "            file_paths.append(os.path.join(subdir, file))\n",
    "\n",
    "print(f\"Found {len(file_paths)} JSON files to process\")\n",
    "\n",
    "# Counter for files with Chinese characters\n",
    "chinese_files_count = 0\n",
    "\n",
    "# Add these configuration parameters at the top of your script\n",
    "REJECT_IF_FIRST_MESSAGE_DUPLICATE = True  # Reject if first message matches another conversation\n",
    "DUPLICATE_THRESHOLD = 1  # Maximum number of duplicate messages allowed per conversation\n",
    "\n",
    "# Initialize tracking collections\n",
    "first_message_hashes = set()  # To track first messages\n",
    "message_hashes = defaultdict(int)  # To count each message occurrence\n",
    "duplicate_counts_first_message = 0\n",
    "duplicate_counts_threshold = 0\n",
    "\n",
    "\n",
    "# Modify the file processing loop\n",
    "for file_path in tqdm(file_paths, desc=\"Processing files\"):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        try:\n",
    "            data = json.load(f)\n",
    "            # Ensure JSON data is a list\n",
    "            if isinstance(data, list) and data:  # Check that data is not empty\n",
    "                has_chinese = False\n",
    "                skip_conversation = False\n",
    "                \n",
    "                # Check if first message is a duplicate\n",
    "                if REJECT_IF_FIRST_MESSAGE_DUPLICATE and data and \"content\" in data[0]:\n",
    "                    first_message_hash = hashlib.md5(data[0][\"content\"].encode('utf-8')).hexdigest()\n",
    "                    if first_message_hash in first_message_hashes:\n",
    "                        duplicate_counts_first_message += 1\n",
    "                        skip_conversation = True\n",
    "                    else:\n",
    "                        first_message_hashes.add(first_message_hash)\n",
    "                \n",
    "                # Count duplicate messages in this conversation\n",
    "                if not skip_conversation:\n",
    "                    duplicate_message_count = 0\n",
    "                    \n",
    "                    # Check each message in the conversation\n",
    "                    for item in data:\n",
    "                        if isinstance(item, dict) and \"content\" in item:\n",
    "                            content_hash = hashlib.md5(item[\"content\"].encode('utf-8')).hexdigest()\n",
    "                            \n",
    "                            if message_hashes[content_hash] > 0:\n",
    "                                duplicate_message_count += 1\n",
    "                            \n",
    "                            message_hashes[content_hash] += 1\n",
    "                    \n",
    "                    # Skip if exceeds duplicate threshold\n",
    "                    if duplicate_message_count > DUPLICATE_THRESHOLD:\n",
    "                        duplicate_counts_threshold += 1\n",
    "                        skip_conversation = True\n",
    "                \n",
    "                # Process the conversation if it wasn't skipped\n",
    "                if not skip_conversation:\n",
    "                    # Process each message\n",
    "                    for item in data:\n",
    "                        if isinstance(item, dict) and \"content\" in item:\n",
    "                            result = process_and_segment_content(item[\"content\"])\n",
    "                            item[\"content\"] = result[\"processed_content\"]\n",
    "                            item[\"language_segments\"] = result[\"language_segments\"]\n",
    "                            \n",
    "                            # Check for Chinese content\n",
    "                            for segment in result[\"language_segments\"]:\n",
    "                                if segment[\"lang\"] == \"zh-cn\":\n",
    "                                    has_chinese = True\n",
    "                    \n",
    "                    all_conversations.append(data)\n",
    "                    \n",
    "                    if has_chinese:\n",
    "                        chinese_files_count += 1\n",
    "            else:\n",
    "                raise ValueError(\"Unexpected JSON format (not a list or empty list)\")\n",
    "        except Exception as e:\n",
    "            # Print error information\n",
    "            print(f\"\\nError processing file: {file_path}\")\n",
    "            print(f\"Error: {e}\")\n",
    "            try:\n",
    "                f.seek(0)\n",
    "                print(f\"Content:\\n{f.read()}\\n\")\n",
    "            except Exception as read_error:\n",
    "                print(f\"Could not read file content: {read_error}\\n\")\n",
    "\n",
    "\n",
    "print(f\"Processing complete. Found {len(all_conversations)} conversations\")\n",
    "print(f\"Files containing Chinese characters: {chinese_files_count}\")\n",
    "\n",
    "# Add these to your final output statistics\n",
    "print(f\"Removed {duplicate_counts_first_message} conversations with duplicate first messages\")\n",
    "print(f\"Removed {duplicate_counts_threshold} conversations exceeding duplicate message threshold\")\n",
    "print(f\"Total removed conversations: {duplicate_counts_first_message + duplicate_counts_threshold}\")\n",
    "\n",
    "# Сохранение обработанных данных в JSONL-файл\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for conversation in tqdm(all_conversations, desc=\"Saving conversations\"):\n",
    "        f.write(json.dumps(conversation, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Processed conversations saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e98ffc32ce372dcb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T18:57:28.867235Z",
     "start_time": "2025-03-07T18:57:28.864949Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'Как можно использовать выражение \"谢谢\" в разных ситуациях\\\\? Есть ли формальные и неформальные варианты благодарности?',\n",
       "  'language_segments': [{'text': 'Как можно использовать выражение \"',\n",
       "    'lang': 'ru'},\n",
       "   {'text': '谢谢', 'lang': 'zh-cn'},\n",
       "   {'text': '\" в разных ситуациях\\\\? Есть ли формальные и неформальные варианты благодарности?',\n",
       "    'lang': 'ru'}]},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Выражение \"谢谢\" (спасибо) можно использовать в различных ситуациях\\\\. Вот несколько примеров: во-первых, неформальная благодарность: когда друг помогает вам: 朋友帮我，我说：谢谢！ при получении подарка: 收到礼物，我说：谢谢！; во-вторых, формальная благодарность: в разговоре с учителем или начальником: 老师帮助我，我说：谢谢您。 при обращении к незнакомцу: 乘客帮我，我说：谢谢您。; в-третьих, дополнительные варианты: \"多谢\" (большое спасибо) — можно использовать в неформальной обстановке, когда хотите выразить большую благодарность\\\\. \"谢谢你\" (спасибо тебе) — более личное, для друзей и близких\\\\. Таким образом, в зависимости от ситуации и уровня формальности, вы можете выбирать подходящие варианты благодарности.',\n",
       "  'language_segments': [{'text': 'Выражение \"', 'lang': 'ru'},\n",
       "   {'text': '谢谢', 'lang': 'zh-cn'},\n",
       "   {'text': '\" (спасибо) можно использовать в различных ситуациях\\\\. Вот несколько примеров: во-первых, неформальная благодарность: когда друг помогает вам:',\n",
       "    'lang': 'ru'},\n",
       "   {'text': '朋友帮我，我说：谢谢！', 'lang': 'zh-cn'},\n",
       "   {'text': 'при получении подарка:', 'lang': 'ru'},\n",
       "   {'text': '收到礼物，我说：谢谢！;', 'lang': 'zh-cn'},\n",
       "   {'text': 'во-вторых, формальная благодарность: в разговоре с учителем или начальником:',\n",
       "    'lang': 'ru'},\n",
       "   {'text': '老师帮助我，我说：谢谢您。', 'lang': 'zh-cn'},\n",
       "   {'text': 'при обращении к незнакомцу:', 'lang': 'ru'},\n",
       "   {'text': '乘客帮我，我说：谢谢您。;', 'lang': 'zh-cn'},\n",
       "   {'text': 'в-третьих, дополнительные варианты: \"', 'lang': 'ru'},\n",
       "   {'text': '多谢', 'lang': 'zh-cn'},\n",
       "   {'text': '\" (большое спасибо) — можно использовать в неформальной обстановке, когда хотите выразить большую благодарность\\\\. \"',\n",
       "    'lang': 'ru'},\n",
       "   {'text': '谢谢你', 'lang': 'zh-cn'},\n",
       "   {'text': '\" (спасибо тебе) — более личное, для друзей и близких\\\\. Таким образом, в зависимости от ситуации и уровня формальности, вы можете выбирать подходящие варианты благодарности.',\n",
       "    'lang': 'ru'}]},\n",
       " {'role': 'user',\n",
       "  'content': 'Можно ли использовать \"谢谢\" в сочетании с другими выражениями\\\\? Например, как выразить благодарность за конкретную помощь или услугу?',\n",
       "  'language_segments': [{'text': 'Можно ли использовать \"', 'lang': 'ru'},\n",
       "   {'text': '谢谢', 'lang': 'zh-cn'},\n",
       "   {'text': '\" в сочетании с другими выражениями\\\\? Например, как выразить благодарность за конкретную помощь или услугу?',\n",
       "    'lang': 'ru'}]},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Да, \"谢谢\" можно использовать в сочетании с другими выражениями для более конкретной благодарности\\\\. Вот несколько примеров: во-первых, благодарность за конкретную помощь: 如果朋友帮你搬家，可以说：谢谢你的帮助！(спасибо за твою помощь!); Во-вторых, благодарность за услугу: 如果有人为你提供了服务，可以说：谢谢你的服务！(спасибо за вашу услугу!); В-третьих, благодарность за советы или поддержку: 如果有人给你建议，可以说：谢谢你的建议！(спасибо за твой совет!); В-четвёртых, благодарность за поддержку в трудной ситуации: 如果朋友在你困难的时候支持你，可以说：谢谢你在我困难时的支持！(спасибо за поддержку в трудные времена!) Таким образом, добавление конкретных деталей делает вашу благодарность более искренней и персонализированной.',\n",
       "  'language_segments': [{'text': 'Да, \"', 'lang': 'ru'},\n",
       "   {'text': '谢谢', 'lang': 'zh-cn'},\n",
       "   {'text': '\" можно использовать в сочетании с другими выражениями для более конкретной благодарности\\\\. Вот несколько примеров: во-первых, благодарность за конкретную помощь:',\n",
       "    'lang': 'ru'},\n",
       "   {'text': '如果朋友帮你搬家，可以说：谢谢你的帮助！', 'lang': 'zh-cn'},\n",
       "   {'text': '(спасибо за твою помощь!); Во-вторых, благодарность за услугу:',\n",
       "    'lang': 'ru'},\n",
       "   {'text': '如果有人为你提供了服务，可以说：谢谢你的服务！', 'lang': 'zh-cn'},\n",
       "   {'text': '(спасибо за вашу услугу!); В-третьих, благодарность за советы или поддержку:',\n",
       "    'lang': 'ru'},\n",
       "   {'text': '如果有人给你建议，可以说：谢谢你的建议！', 'lang': 'zh-cn'},\n",
       "   {'text': '(спасибо за твой совет!); В-четвёртых, благодарность за поддержку в трудной ситуации:',\n",
       "    'lang': 'ru'},\n",
       "   {'text': '如果朋友在你困难的时候支持你，可以说：谢谢你在我困难时的支持！', 'lang': 'zh-cn'},\n",
       "   {'text': '(спасибо за поддержку в трудные времена!) Таким образом, добавление конкретных деталей делает вашу благодарность более искренней и персонализированной.',\n",
       "    'lang': 'ru'}]}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_conversations[2001]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
